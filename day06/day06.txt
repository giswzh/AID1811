Day05回顾
1、Ajax动态加载
  1、抓包工具或者F12抓参数 ：QueryString
  2、params = {QuerySting中一堆查询参数}
  3、URL地址 ：抓包工具/F12抓到的GET的地址
2、selenium+phantomjs
  1、selenium ：自动化测试工具
  2、phantomjs：无界面浏览器/无头浏览器
  3、使用流程
    * from selenium import webdriver
    * browser=webdriver.PhantomJS(executable_path=)
    * browser.get('URL')
    * ele = browser.find_element_by_class_name('')
    * ele.send_keys('')
    * ele.click()
    * browser.switch_to.frame(节点对象)
    * browser.close()
    * browser.quit()
  4、常用方法
    * browser.get('URL')
    * browser.page_source
    * browser.page_source.find('查找的字符串')
      -1 ：未找到
    * browser.find_elements_by_xpath('')
    * 节点对象.text
3、selenium+chromedriver
  1、下载对应版本的chromedriver
  2、将chromedriver.exe放到python安装目录Scripts下
  3、设置无界面模式(方法1)
     * options = webdriver.ChromeOptions()
     * options.set_headless()
     * browser = webdriver.Chrome(options=options)
  4、设置无界面模式(方法2)
     * options = webdriver.ChromeOptions()
     * options.add_argument('--headless')
     * browser = webdriver.Chrome(options=options)
**************************************************
Day06笔记
1、豆瓣爬虫
  * 输出字体变色,一般用于菜单
    print('\033[40m九霄龙吟惊天变\033[0m')
    print('九霄\033[31m龙吟\033[0m惊天变')
2、京东爬虫
   * 商品信息 ： //div[@id="J_goodsList"]//li
   * 搜索框   ： class属性值为 text
   * 搜索按钮 ： class属性值为 button
   * 下一页(能点) ：class属性值为 pn-next
   * 下一页(不能点)：class属性值为pn-next disabled
3、selenium+chromedriver如何设置代理IP
   * options = webdriver.ChromeOptions()
   * options.add_argument('--proxy-server=http://')
   * browser = webdriver.Chrome(options=options)
4、等待页面加载方式
  1、强制等待(time.sleep(n))
    * 缺点 ：不智能,设置时间短导致有些元素加载不出来,时间长则浪费时间
  2、隐性等待(可以) ：driver.implicitly_wait(n)
    * 定义:设置时间,指定时间内完成,则下一步,否则抛出异常
    * 特点:全局性,程序开头设置,整个程序有效
    * 缺点:整个页面未加载完,我们需要的元素已加载,还得继续等
  3、显性等待(推荐) ：WebDriverWait
    * 定义:程序每隔0.5秒检查,若成立,则下一步,超过最长时间会抛出异常
5、多线程爬虫
  1、应用场景
     ** 多进程 ：大量密集计算
     ** 多线程 ：IO操作(本地磁盘IO、网络IO)
  2、原理 ：见图
  3、知识点
     1、队列(from multiprocessing import Queue)
        * q = Queue()
	* q.put(url)
	* q.get()   ：队列为空时会阻塞
	**q.get(block=True,timeout=1) ：超时抛异常
	* q.empty() ：空(True) 非空(False)
     2、线程模块(from threading import Thread)
        * t = Thread(target=函数名)
	* t.start()
	* t.join()
     3、线程使用示例
	threads = []	
	for i in range(3):
	    t = Thread(target=get_page)
	    threads.append(t)
	    t.start()

	for t in threads:
	    t.join()
6、小米应用商店数据抓取(多线程)
  1、URL ：百度搜 小米应用商店
  2、目标：应用分类(聊天社交)
     * 应用名称
     * 应用链接
  3、抓JSON地址及查询参数：F12或者抓包工具
     * GET地:http://app.mi.com/categotyAllListApi?
     * 查询参数
	params = {
	   'page' : ??,
	   'categoryId'	: '2',
	   'pageSize' : '30'
	}
7、BeautifulSoup解析模块
  1、定义 ：HTML和XML的解析器,依赖于lxml
  2、安装 ：sudo pip3 install beautifulsoup4
  3、使用流程
     * from bs4 import BeautifulSoup
     * soup = BeautifulSoup(html,'lxml')
     * r_list = soup.find_all(条件)
  4、BeautifulSoup支持的解析库
     * lxml ：速度快,文档容错能力强
     * html.parser ：标准库,都一般
     * xml  ：速度快,文档容错能力强
8、链家二手房数据抓取
  1、URL ：链家二手房 - 二手房
  2、目标
     * 名称
     * 户型
     * 面积
     * 楼层
     * 年份
     * 地点
     * 单价
     * 总价
  3、思路
    1、获取所有房源的节点对象列表
    2、用每个房源节点对象继续调用find()查找指定信息节点对象并获取文本内容
    3、抓下来定义字典,存到mongodb数据库

      *********用多线程实现*********
      *********参考xpath思路********
      *********安装Scrapy框架*******







